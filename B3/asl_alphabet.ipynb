{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9ba8a7",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77e323b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ADMIN\\.cache\\kagglehub\\datasets\\grassknoted\\asl-alphabet\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "DATA_DIR = os.path.join(\n",
    "    path,\n",
    "    \"asl_alphabet_train\",\n",
    "    \"asl_alphabet_train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9dc016",
   "metadata": {},
   "source": [
    "# Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e209b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1a408",
   "metadata": {},
   "source": [
    "# Chuan hoa du lieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9288f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # chuyen doi gtri [0,255] ve [0,1]\n",
    "    validation_split=0.2 # Chia du lieu thanh 80% train va 20% validation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6915ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69623 images belonging to 2 classes.\n",
      "Found 17405 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = datagen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\" # chi dinh lay tap train\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    path,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\" # chi dinh lay tap validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e8f44",
   "metadata": {},
   "source": [
    "# Khai bao mo hinh - fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb2a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Conv2D name=conv2d, built=False>,\n",
       " <MaxPooling2D name=max_pooling2d, built=True>,\n",
       " <Conv2D name=conv2d_1, built=False>,\n",
       " <MaxPooling2D name=max_pooling2d_1, built=True>,\n",
       " <Conv2D name=conv2d_2, built=False>,\n",
       " <MaxPooling2D name=max_pooling2d_2, built=True>,\n",
       " <Flatten name=flatten, built=False>,\n",
       " <Dense name=dense, built=False>,\n",
       " <Dropout name=dropout, built=True>,\n",
       " <Dense name=dense_1, built=False>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential\n",
    "(\n",
    "    [\n",
    "        keras.layers.Conv2D(32,(3,3), activation=\"relu\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        \n",
    "        keras.layers.Conv2D(64,(3,3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        \n",
    "        keras.layers.Conv2D(128,(3,3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D((2,2)),\n",
    "        \n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(512, activation=\"relu\"), # fully connected layer\n",
    "        keras.layers.Dropout(0.5), # dropout de tranh overfitting\n",
    "        keras.layers.Dense(29,activation=\"softmax\") # 29 classes trong asl\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f9245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(64,64,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(train_data.num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4d42bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 198ms/step - accuracy: 0.9996 - loss: 0.0102 - val_accuracy: 0.9997 - val_loss: 0.0031\n",
      "Epoch 2/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m443s\u001b[0m 203ms/step - accuracy: 0.9997 - loss: 0.0039 - val_accuracy: 0.9997 - val_loss: 0.0074\n",
      "Epoch 3/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 163ms/step - accuracy: 0.9997 - loss: 0.0042 - val_accuracy: 0.9997 - val_loss: 0.0033\n",
      "Epoch 4/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 185ms/step - accuracy: 0.9997 - loss: 0.0034 - val_accuracy: 0.9997 - val_loss: 0.0061\n",
      "Epoch 5/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 194ms/step - accuracy: 0.9997 - loss: 0.0033 - val_accuracy: 0.9997 - val_loss: 0.0032\n",
      "Epoch 6/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 177ms/step - accuracy: 0.9997 - loss: 0.0032 - val_accuracy: 0.9997 - val_loss: 0.0046\n",
      "Epoch 7/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 177ms/step - accuracy: 0.9997 - loss: 0.0034 - val_accuracy: 0.9997 - val_loss: 0.0031\n",
      "Epoch 8/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 186ms/step - accuracy: 0.9997 - loss: 0.0031 - val_accuracy: 0.9997 - val_loss: 0.0037\n",
      "Epoch 9/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 186ms/step - accuracy: 0.9997 - loss: 0.0032 - val_accuracy: 0.9997 - val_loss: 0.0026\n",
      "Epoch 10/10\n",
      "\u001b[1m2176/2176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 212ms/step - accuracy: 0.9997 - loss: 0.0029 - val_accuracy: 0.9997 - val_loss: 0.0026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30752</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,936,384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30752\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,936,384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,812,616</span> (45.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,812,616\u001b[0m (45.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,937,538</span> (15.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,937,538\u001b[0m (15.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,875,078</span> (30.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m7,875,078\u001b[0m (30.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# huan luyen\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=val_data\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360170e",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47659a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"asl_alphabet_model.h5\") # .h5 dung de load lai va du doan cho sau nay\n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167245e",
   "metadata": {},
   "source": [
    "# Danh gia mo hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7506a022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 39ms/step - accuracy: 0.9997 - loss: 0.0026\n",
      "Accuracy: 0.9997\n",
      "Loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_data)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42722715",
   "metadata": {},
   "source": [
    "# Du doan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58716e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load mo hinh\n",
    "loaded_model = keras.models.load_model(\"asl_alphabet_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe7b8fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asl_alphabet_test', 'asl_alphabet_train']\n"
     ]
    }
   ],
   "source": [
    "# chuan bi label {chu cai}\n",
    "class_names = list(train_data.class_indices.keys())\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2e6c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 1 hinh de xu ly\n",
    "img_dir = os.path.join(path, \"asl_alphabet_train\", \"asl_alphabet_train\", \"M\")  # duong dan den thu muc chua hinh\n",
    "# Get first image file\n",
    "img_file = os.listdir(img_dir)[0]\n",
    "img_path = os.path.join(img_dir, img_file) # duong dan den hinh can du doan\n",
    "# img_path = input(\"Enter the path of the image to predict: \")\n",
    "img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = img_array / 255.0  # scale anh\n",
    "img_array = np.expand_dims(img_array, axis=0)  # them kich thuoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f20f7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "[[0.00708671 0.9929133 ]]\n",
      "Predicted label: asl_alphabet_train\n"
     ]
    }
   ],
   "source": [
    "# du doan\n",
    "prediction = loaded_model.predict(img_array)\n",
    "predicted_index = np.argmax(prediction)\n",
    "predicted_label = class_names[predicted_index]\n",
    "\n",
    "# print % cac lop du doan\n",
    "print(prediction)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
